{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Memex","text":"<p>This is the documentation for the Memex project.</p>"},{"location":"NEXT_STEPS/","title":"Memex Project: Next Steps","text":"<p>This document outlines a phased development plan to build the Memex application as envisioned in the architectural-outline.md. The project currently consists of a new React Native application, and the following steps will guide the implementation of the backend services and the connection between the frontend and backend.</p>"},{"location":"NEXT_STEPS/#phase-1-foundational-setup-core-backend","title":"Phase 1: Foundational Setup &amp; Core Backend","text":"<p>Goal: Establish the project structure, set up the development environment, and build the initial version of the Go backend service.</p> <ol> <li> <p>Setup Go Backend Monorepo:</p> <ul> <li>Create a <code>backend/</code> directory in the root of the project.</li> <li>Inside <code>backend/</code>, initialize a Go module: <code>go mod init github.com/sheldoncross/memex/backend</code>.</li> </ul> </li> <li> <p>Define Core API Contract:</p> <ul> <li>Start with a simple REST API for the first features. Define an OpenAPI (Swagger) specification for the initial endpoints.</li> <li>Key initial endpoint: <code>POST /api/v1/nodes</code> for ingesting new notes.</li> </ul> </li> <li> <p>Implement Backend Server &amp; LLM Service Wrapper:</p> <ul> <li>In the <code>backend/</code> directory, set up a basic HTTP server using a standard library or a lightweight framework like Gin.</li> <li>Create an <code>llm/</code> package that acts as a wrapper for an external LLM provider (e.g., Gemini or OpenAI API). This service will be responsible for generating vector embeddings from text.</li> </ul> </li> <li> <p>Select and Integrate a Vector Database:</p> <ul> <li>As per the architecture, a vector database is essential. For rapid development, start with a managed service like Pinecone or a self-hostable option like Weaviate or Qdrant.</li> <li>Create a <code>database/</code> package in the Go backend that handles the connection and data access logic (e.g., <code>StoreNode(text, vector)</code>).</li> </ul> </li> <li> <p>Containerize the Development Environment:</p> <ul> <li>Create a <code>docker-compose.yml</code> file in the project root to manage the Go backend and the chosen database service. This ensures a consistent and reproducible development environment for all contributors.</li> </ul> </li> </ol>"},{"location":"NEXT_STEPS/#phase-2-implement-first-end-to-end-feature-data-ingestion","title":"Phase 2: Implement First End-to-End Feature (Data Ingestion)","text":"<p>Goal: Implement the \"Ingesting Data\" flow described in the architectural outline, creating the first fully functional feature.</p> <ol> <li> <p>Build the Data Ingestion Endpoint:</p> <ul> <li>Implement the logic for the <code>POST /api/v1/nodes</code> endpoint in the Go backend.</li> <li>The handler should:</li> <li>Receive text content from the request body.</li> <li>Call the LLM service to generate a vector embedding for the text.</li> <li>Call the database service to store the original text and its corresponding vector.</li> </ul> </li> <li> <p>Develop UI for Note Creation:</p> <ul> <li>In the React Native application (<code>App.tsx</code> to start), create a simple UI with:</li> <li>A <code>TextInput</code> for writing a new note.</li> <li>A \"Save\" <code>Button</code>.</li> </ul> </li> <li> <p>Connect Frontend to Backend:</p> <ul> <li>Create a service client in the React Native app (e.g., in <code>src/services/api.ts</code>) to handle HTTP requests to the Go backend.</li> <li>When the user presses \"Save\", the app should call the <code>POST /api/v1/nodes</code> endpoint with the note's content.</li> <li>Implement basic state management (e.g., using Zustand or Redux Toolkit) to handle loading and success/error states.</li> </ul> </li> </ol>"},{"location":"NEXT_STEPS/#phase-3-implement-core-retrieval-features","title":"Phase 3: Implement Core Retrieval Features","text":"<p>Goal: Enable users to retrieve and see connections between their notes, bringing the \"memex\" concept to life.</p> <ol> <li> <p>Implement Semantic Search:</p> <ul> <li>Backend: Create a new endpoint: <code>GET /api/v1/search?q={query}</code>.</li> <li>This endpoint will take a search query, generate an embedding for it, and use the vector database to find and return the most semantically similar notes.</li> <li>Frontend: Add a search bar to the UI. As the user types, call the search endpoint and display the results.</li> </ul> </li> <li> <p>Develop \"Associative Trails\":</p> <ul> <li>Backend: Create an endpoint: <code>GET /api/v1/nodes/{nodeId}/related</code>.</li> <li>When a user views a single note, this endpoint will be called. It will use the note's vector to find similar items in the database.</li> <li>Frontend: When viewing a note, fetch and display a list of these related notes, forming the first version of an \"associative trail.\"</li> </ul> </li> <li> <p>Build the Main Notes View:</p> <ul> <li>Create a new endpoint <code>GET /api/v1/nodes</code> to retrieve all notes.</li> <li>Develop the main screen of the app to display the list of all saved notes, allowing a user to tap on one to view its details and its associative trail.</li> </ul> </li> </ol>"},{"location":"NEXT_STEPS/#phase-4-advanced-features-refinement","title":"Phase 4: Advanced Features &amp; Refinement","text":"<p>Goal: Enhance the core features with more advanced LLM capabilities and prepare for a wider audience.</p> <ol> <li> <p>Implement Natural Language Query:</p> <ul> <li>As outlined in the architecture, enhance the search endpoint to accept complex natural language queries.</li> <li>The backend will pass the query to the LLM with a prompt engineered to deconstruct it into keywords and logical operators, which can then be used to perform more sophisticated searches against the database.</li> </ul> </li> <li> <p>Integrate a Graph Database:</p> <ul> <li>To manage explicit relationships, integrate a graph database like Neo4j or Dgraph.</li> <li>When creating associative trails, store the discovered links as edges in the graph. This allows for more permanent and complex relationship traversal than relying solely on vector similarity for every query.</li> </ul> </li> <li> <p>On-Device LLM Integration:</p> <ul> <li>Begin research and development into integrating a self-hosted or on-device model (e.g., using Ollama with Phi-3) for privacy-focused tasks like tagging or summarization, as outlined in the hybrid approach.</li> </ul> </li> <li> <p>UI/UX Polish:</p> <ul> <li>Refine the user interface. Consider a more visual way to represent the \"associative trails,\" perhaps using a graph visualization library.</li> <li>Improve the overall user experience based on</li> </ul> </li> </ol>"},{"location":"architectural-outline/","title":"Architecture","text":"<pre><code>**Memex Application: Architectural Outline**\n\nVersion: 1.1\n\nDate: 2025-08-04\n\nAuthor: Gemini\n\n**1. High-Level System Overview &amp; Goals**\n\n**1.1. Problem Statement**\n\nIn the current digital landscape, information is abundant but fragmented across various applications and platforms. Traditional hierarchical file systems (folders within folders) are rigid and fail to capture the associative, non-linear way the human mind works. This leads to information silos, difficulty in rediscovering knowledge, and a cognitive overhead in managing personal and professional data.\n\n**1.2. Solution Vision**\n\nTo develop a portable, cross-platform application inspired by Vannevar Bush's \"memex\" concept. The application will provide a unified, filesystem-like interface to a user's knowledge base. By leveraging Large Language Models (LLMs), it will move beyond simple storage to create a dynamic, self-organizing system that builds \"associative trails\" between disparate pieces of information, mirroring human thought processes.\n\n**1.3. Key Goals**\n\n*   **Natural Organization:** Automatically categorize, tag, and link information based on semantic meaning, not just file names or folder locations.\n*   **Associative Trails:** Proactively and on-demand, create and visualize connections between notes, documents, images, and web clippings, forming a personal knowledge graph.\n*   **Cross-Platform Portability:** Deliver a seamless and consistent user experience across all major platforms (iOS, Android, macOS, Windows, Linux) from a single, manageable codebase.\n*   **Natural Language Interface:** Allow users to query their knowledge base using conversational language (e.g., \"Find my notes on the braking systems of GT3 cars and that article about the 24 Hours of Le Mans\").\n\n**2. Architectural Style &amp; Core Components**\n\n**2.1. Architectural Pattern**\n\nA **Layered Architecture** (specifically, a form of Clean Architecture) is proposed. This pattern decouples the core business logic from external concerns like the UI, database, and third-party services.\n\n**Justification:**\n\n*   **Portability:** The core logic remains independent of any specific UI framework or platform, making it easier to support multiple frontends.\n*   **Testability:** Each layer can be tested in isolation, improving code quality and reliability.\n*   **Maintainability:** Clear separation of concerns makes the system easier to understand, modify, and scale.\n\n**2.2. Core Components &amp; Responsibilities**\n\n</code></pre> <p>+---------------------------------------------------+ |               Presentation Layer                  | (React Native) +---------------------------------------------------+ | v +---------------------------------------------------+ |                 Core Logic Layer                  | (Go) | (Knowledge Graph Mgmt, File Ops, Trail Logic)     | +----------------------+----------------------------+ |                            | v                            v +----------------------+             +-------------------------+ |      Data Layer      |             |       LLM Service       | | (Graph/Vector DB)    |             | (API Wrapper / Local)   | +----------------------+             +-------------------------+ ^ | +---------------------------------------------------+ |             Platform-Specific Layer               | (Native Code Bridge) +---------------------------------------------------+</p> <pre><code>\n*   **Presentation Layer (UI/UX):**\n    *   **Responsibility:** Renders the user interface, captures user input, and displays data from the Core Logic Layer.\n    *   **Considerations:** Must be built using a cross-platform framework to maintain a single codebase. It is stateless and dumb, simply reflecting the state provided by the core.\n*   **Core Logic Layer (Business Logic):**\n    *   **Responsibility:** The \"brain\" of the application. It is written in pure, dependency-free code.\n    *   **Details:**\n        *   Manages the knowledge graph entity relationships.\n        *   Handles abstract file-like operations (create, read, link, query).\n        *   Contains the logic for generating and traversing associative trails.\n        *   Orchestrates calls to the Data Layer and LLM Service.\n*   **Data Layer (Persistence):**\n    *   **Responsibility:** Abstracting data storage and retrieval.\n    *   **Details:** Implements the interface defined by the Core Logic Layer for persisting nodes (information chunks) and edges (associations). This could be a graph database, a vector database, or a combination.\n*   **LLM Service:**\n    *   **Responsibility:** A dedicated component that acts as a gateway for all LLM interactions.\n    *   **Details:**\n        *   Exposes simple methods to the Core Logic Layer (e.g., generateTags(text), findConnections(nodeA, nodeB), answerQuery(query)).\n        *   Handles prompt engineering, API calls to external services, or inference with a local model.\n*   **Platform-Specific Layer:**\n    *   **Responsibility:** A thin wrapper that bridges the cross-platform code with native OS functionalities.\n    *   **Details:** Handles file system access, system notifications, deep linking, and other OS-specific APIs, translating them into commands the Core Logic Layer can understand.\n\n**3. Technology Stack Recommendations**\n\n**3.1. Cross-Platform Framework (Presentation Layer)**\n\n*   **Recommendation:** **React Native**.\n*   **Justification:** Leverages the popular React library, making it a natural fit for developers with a web background. It has a massive ecosystem of libraries and community support. The ability to share code with a web application via React Native for Web is a significant advantage for future expansion.\n\n**3.2. Backend &amp; LLM Integration**\n\n*   **LLM:**\n    *   **Recommendation:** **Hybrid Approach.** Start with an **External API (e.g., Gemini, OpenAI API)** for rapid prototyping and powerful capabilities. Plan for future integration of smaller, **Self-Hosted/On-Device Models** (e.g., using Ollama with Llama 3 or Phi-3) for privacy-sensitive, low-latency tasks like basic tagging.\n    *   **Trade-offs:**\n        *   **API:** Higher performance, but with costs, latency, and data privacy concerns.\n        *   **Self-Hosted:** Full data privacy and no direct cost, but requires more powerful hardware and has lower performance.\n*   **Language/Framework (Core Logic &amp; Services):**\n    *   **Recommendation:** **Go (Golang)**.\n    *   **Justification:** Go is exceptionally well-suited for building scalable, high-performance network services and APIs. Its built-in support for concurrency (goroutines and channels) is perfect for handling concurrent LLM requests and database interactions efficiently. Its simplicity, fast compile times, and single binary deployment streamline development and operations.\n*   **Database (Data Layer):**\n    *   **Recommendation:** **Combination of Graph and Vector Database.**\n    *   **Details:**\n        *   **Graph DB (e.g., Neo4j, Dgraph):** Ideal for storing the explicit relationships (the \"trails\") between information nodes. Dgraph, being built in Go, could be a particularly good fit.\n        *   **Vector DB (e.g., Pinecone, Weaviate):** Essential for storing vector embeddings generated by the LLM. This enables powerful semantic search and similarity-based link discovery.\n\n**4. Data Flow &amp; Key Interactions**\n\n**4.1. Flow 1: Ingesting Data (e.g., a new note)**\n\n1.  **User Action:** User creates a new note in the React Native app.\n2.  **Core Logic:** The UI sends the raw text to the Go-based Core Logic Layer (likely via a REST or gRPC API).\n3.  **LLM Processing:** The Core Logic Layer passes the text to the LLM Service, requesting semantic tags, a summary, and vector embeddings.\n4.  **Data Storage:** The Core Logic Layer instructs the Data Layer to:\n    *   Store the note content, tags, and summary as a new Node in the Graph DB.\n    *   Store the generated vector embedding in the Vector DB, linked to the new Node's ID.\n5.  **Trail Discovery:** (Asynchronously) The Core Logic can query the Vector DB to find semantically similar nodes and create new relationship Edges in the Graph DB.\n\n**4.2. Flow 2: Creating an Associative Trail**\n\n1.  **User Action:** User is viewing a Node (e.g., a note about \"Succession\").\n2.  **Core Logic:** The Core Logic Layer queries the Data Layer: \"Find all nodes connected to this 'Succession' node in the Graph DB.\"\n3.  **LLM Enhancement:** For deeper connections, the Core Logic can ask the LLM Service: \"Given this text about 'Succession', what other topics in my database might be related?\" The LLM uses its embeddings to suggest links (e.g., to notes about \"King Lear\" or \"corporate strategy\").\n4.  **UI Display:** The Presentation Layer receives the list of direct and suggested links from the Go backend and visualizes them as a \"trail.\"\n\n**4.3. Flow 3: Natural Language Query**\n\n1.  **User Action:** User types \"Find all my notes about F1 racing and Breaking Bad\" into the search bar.\n2.  **Core Logic &amp; LLM:** The query is sent to the LLM Service. The LLM deconstructs the\n\n</code></pre>"}]}